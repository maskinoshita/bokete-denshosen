{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Host a Boke AI Keras Model on Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker is a fully-managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK makes it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks, including TensorFlow and Keras.\n",
    "\n",
    "In this notebook, we train and host a Keras boke-AI model on SageMaker. The model used for this notebook is a neural network (CNN and Bidirectional-LSTM) for image captioning that was developed by Ryuichi Ishikawa (Dentsu Digital).\n",
    "\n",
    "<br>\n",
    "<b> Instance Type and Pricing:  </b>\n",
    "\n",
    "Before running this notebook, you should request quota increases for the number of instance in your AWS account [[How to request a quota increase (Japanese)](../../LIMIT_INCREASE.md)]. This sample notebook was tested using \n",
    "- the `conda_tensorflow2_p36` kernel on `ml.m5.xlarge` (notebook instance) or `Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)` kernel `ml.m5.large` (Studio notebook) \n",
    "- with the `ml.g4dn.xlarge` processing instance type, \n",
    "- `ml.g4dn.2xlarge` training instance type, \n",
    "- and `ml.g4dn.xlarge` hosting instance type \n",
    "\n",
    "in the `us-west-2`, `us-east-1`, `us-east-2`, and `ap-northeast-1` regions. The processing and training time are approximately 7 minutes and 13 minuites, respectively, using a subset of dataset only containing 8k image/text pairs on the aforementioned hardware specifications.\n",
    "\n",
    "Price per hour depends on your region and instance type. You can reference prices on the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/).  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we define a few variables that are be needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap-northeast-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# from sagemaker import Session\n",
    "# from sagemaker import get_execution_role\n",
    "\n",
    "# sagemaker_session = Session()\n",
    "# default_bucket = sagemaker_session.default_bucket()\n",
    "# role = get_execution_role()\n",
    "\n",
    "import boto.utils\n",
    "meta = boto.utils.get_instance_metadata()\n",
    "region_name = meta['placement']['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting mecab-python3\n",
      "  Downloading mecab_python3-1.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.3/577.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mecab-python3\n",
      "Successfully installed mecab-python3-1.0.5\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting unidic\n",
      "  Using cached unidic-1.1.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from unidic) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from unidic) (4.62.3)\n",
      "Collecting wasabi<1.0.0,>=0.6.0\n",
      "  Using cached wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting plac<2.0.0,>=1.1.3\n",
      "  Using cached plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->unidic) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->unidic) (2022.5.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->unidic) (3.1)\n",
      "Building wheels for collected packages: unidic\n",
      "  Building wheel for unidic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7426 sha256=c13db542b426fd0a663b4a2bb23a0910bb300be1147ca8bcdc530fdcc2851048\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/bc/bb/46aba36d0388f67dfe44bb0edc20a2c964560d4d19ec394e05\n",
      "Successfully built unidic\n",
      "Installing collected packages: wasabi, plac, unidic\n",
      "Successfully installed plac-1.3.5 unidic-1.1.0 wasabi-0.9.1\n",
      "download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic-3.1.0.zip\n",
      "Dictionary version: 3.1.0+2021-08-31\n",
      "Downloading UniDic v3.1.0+2021-08-31...\n",
      "unidic-3.1.0.zip: 100%|██████████████████████| 526M/526M [00:06<00:00, 80.9MB/s]\n",
      "Finished download.\n",
      "Downloaded UniDic v3.1.0+2021-08-31 to /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/unidic/dicdir\n"
     ]
    }
   ],
   "source": [
    "# added: install Mecab\n",
    "!pip install mecab-python3\n",
    "!pip install unidic\n",
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokete Bokekan dataset\n",
    "\n",
    "Bokete is one of the most popular commedy web site. [Bokekan dataset](../README.md) consists of 1M+ images/text pairs to 4 different classes. Here are the classes in the dataset, as well as 1 random image/text pair:\n",
    "\n",
    "| class | boke (image/text pair) | number of stars |\n",
    "| ---- | ----: | ---- |\n",
    "| blue | 98,736 | 0 |\n",
    "| yellow | 955,901 | 1 - 100 |\n",
    "| green | 37,342 | 101 - 1000 |\n",
    "| red | 8,183 | 1001 - 10000 |\n",
    "| sp | 380 | 10001+ |\n",
    "| Total | 1,100,542 | boke |\n",
    "\n",
    "<div align=\"center\"><img src=\"https://d1.awsstatic.com/Developer%20Marketing/jp/magazine/2020/img_bokete_01.a9c39e30940cf3c6fb5ba795b9202a57a843da05.jpg\" alt=\"bokete\" width=\"320\"/></div>\n",
    "<div align=\"center\">(Photo by IHA Central Office, licensed under the Creative Commons Attribution License 2.0)</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will copy the Bokekan dataset to your SageMaker default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokekan_bucket_name = 'bokekan-{}'.format(region_name)\n",
    "prefix = '21-6-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokekan_list = ['blue_000', 'yellow_000', 'yellow_001', 'yellow_002', 'yellow_003','yellow_004', 'yellow_005', 'yellow_006','yellow_007', 'yellow_008', 'yellow_009', 'green_000', 'red_000', 'sp_000']\n",
    "bokekan = bokekan_list[-2] # Bokekan Red\n",
    "data_path = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 1.63 s, total: 3.71 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "key = bokekan + '.zip'\n",
    "# print(bokekan_bucket_name, '=>', os.path.join(default_bucket, bokekan_bucket_name, prefix, key))\n",
    "\n",
    "# s3.copy_object(\n",
    "#     CopySource=os.path.join(bokekan_bucket_name, prefix, key), \n",
    "#     Bucket=default_bucket, \n",
    "#     Key=os.path.join(bokekan_bucket_name, prefix, key), \n",
    "# )\n",
    "\n",
    "with open(key, 'wb') as f:\n",
    "    s3.download_fileobj(bokekan_bucket_name, os.path.join(prefix, key), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for training\n",
    "\n",
    "To use the bokekan dataset, we first download it and convert it to numpy array. This step takes around 7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize script/processing/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.tensorflow import TensorFlowProcessor\n",
    "\n",
    "# tags = [{'Key': 'Project', 'Value': 'bokete'}]\n",
    "\n",
    "# tf_processor = TensorFlowProcessor(\n",
    "#     framework_version=\"2.3\",\n",
    "#     role=role,\n",
    "#     instance_type=\"ml.g4dn.xlarge\", \n",
    "#     instance_count=1,\n",
    "#     volume_size_in_gb=100, \n",
    "#     base_job_name='bokete-tf-keras-preprocessing', \n",
    "#     py_version='py37', \n",
    "#     tags=tags\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.processing import ProcessingInput, ProcessingOutput, FeatureStoreOutput\n",
    "\n",
    "# # extract_destination = \"s3://{}/{}/{}/{}\".format(default_bucket, bokekan_bucket_name, prefix, \"contents\")\n",
    "\n",
    "# tf_processor.run(\n",
    "#     code=\"preprocessing.py\",\n",
    "#     source_dir=\"script/processing\", \n",
    "#     inputs=[\n",
    "#         ProcessingInput(\n",
    "#             input_name='bokekan',\n",
    "#             source=\"s3://{}/{}/{}\".format(default_bucket, bokekan_bucket_name, prefix), \n",
    "#             destination=\"/opt/ml/processing/input\", \n",
    "#         ),\n",
    "#     ],\n",
    "#     outputs=[\n",
    "#         ProcessingOutput(\n",
    "#             output_name=\"bokekan_contents\", \n",
    "#             source=\"/opt/ml/processing/output/bokekan/content\", \n",
    "# #             destination=extract_destination, \n",
    "#             s3_upload_mode=\"Continuous\", \n",
    "#         ),\n",
    "#         ProcessingOutput( # files: params, word_id, id_word\n",
    "#             output_name=\"bokekan_metadata\", \n",
    "#             source=\"/opt/ml/processing/output/bokekan/metadata\", \n",
    "# #             feature_store_output=FeatureStoreOutput(FeatureGroupName='')\n",
    "#         ),\n",
    "#         ProcessingOutput( # files: X1, X2, y\n",
    "#             output_name=\"training\", \n",
    "#             source=\"/opt/ml/processing/output/training\", \n",
    "#         ),\n",
    "#     ],\n",
    "#     arguments=[\"--bokekan\", bokekan] \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load data ----\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './processing/output/bokekan/contents/red_000.zip/boke.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7993/3120374129.py\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---- Load data ----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessing_bokekan_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boke.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7993/3120374129.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(bokekan)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessing_bokekan_contents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboke_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boke.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdf_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bokekan_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboke_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './processing/output/bokekan/contents/red_000.zip/boke.csv'"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from zipfile import ZipFile\n",
    "import boto3\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import MeCab\n",
    "import unidic # added\n",
    "\n",
    "from tensorflow.keras.models              import Model\n",
    "from tensorflow.keras.applications.vgg16  import VGG16 \n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow import reshape, stack, convert_to_tensor\n",
    "\n",
    "# bokekan = ['blue_000', 'yellow_000', 'yellow_001', 'yellow_002', 'yellow_003','yellow_004', 'yellow_005', 'yellow_006','yellow_007', 'yellow_008', 'yellow_009', 'green_000', 'red_000', 'sp_000']\n",
    "\n",
    "processing_input = '.' # zipped file\n",
    "processing_bokekan_contents = './processing/output/bokekan/contents' # intermediate data (raw data)\n",
    "processing_bokekan_metadata = './processing/output/bokekan/metadata' # Pandas DF -> CSV\n",
    "processing_output = './processing/output/training' # feature vector for model input\n",
    "\n",
    "def load_data(bokekan):\n",
    "    \"\"\"Load Bokekan dataset.\n",
    "\n",
    "    Keyword arguments:\n",
    "    bokekan -- the list of bokekan datasets. e.g. ['green_000', 'red_000', 'sp_000']\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for boke_class in bokekan:\n",
    "        file_path = os.path.join(processing_input, boke_class+'.zip')\n",
    "\n",
    "        try:\n",
    "            zipfile = ZipFile(file_path)\n",
    "            print('Extracting...', file_path)\n",
    "            zipfile.extractall(path=os.path.join(processing_bokekan_contents, boke_class))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        df_ = pd.read_csv(os.path.join(processing_bokekan_contents, boke_class, 'boke.csv'), lineterminator='\\n')\n",
    "        df_['bokekan_id'] = boke_class\n",
    "        df_list.append(df_)\n",
    "        \n",
    "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--bokekan\", \n",
    "# #                         action=\"extend\", # >= py38\n",
    "#                         nargs=\"+\", type=str)\n",
    "#     args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('---- Load data ----')\n",
    "        \n",
    "    data = load_data([ os.path.splitext(os.path.basename(key))[0] ])\n",
    "    data.to_csv(os.path.join(processing_bokekan_metadata, 'boke.csv'), index=False)\n",
    "    \n",
    "    print('---- Text preprocessing ----')\n",
    "    \n",
    "    # delete unnecessary characters\n",
    "    data[\"text\"] = data.text.str.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # remove: already run\n",
    "    # # word tokenization\n",
    "    # subprocess.run(['python', '-m', 'unidic', 'download'])\n",
    "    \n",
    "    tagger = MeCab.Tagger('-Owakati')\n",
    "\n",
    "    w_ys = []\n",
    "\n",
    "    max_word = 0\n",
    "    # put Beginning Of Sentence (BOS) and End Of Sentence (EOS)\n",
    "    for txt in data['text']:\n",
    "        ys = [\"BOS\"]\n",
    "        ys.extend(tagger.parse(txt).replace(\"\\n\", \"\").split())\n",
    "        ys.append(\"EOS\")\n",
    "        w_ys.append(ys)\n",
    "\n",
    "        # check the max word length\n",
    "        if max_word < len(ys):\n",
    "            max_word = len(ys)\n",
    "    \n",
    "    # create the index of words\n",
    "    corpus = []\n",
    "    for word in w_ys:\n",
    "        corpus.extend(word)\n",
    "\n",
    "    # create list of tuples in descending order of frequency\n",
    "    # collections.Counter(corpus).items(): Dictionay {key: word, value: frequency} -> [(key, val), (key, val), ...]\n",
    "    # key=lambda x: x[1]: `sorted` makes `val` sort key\n",
    "    word_id = [(k, v) for k, v in sorted(collections.Counter(corpus).items(), key=lambda x: x[1], reverse=True) if k!=\"EOS\"]\n",
    "\n",
    "    # Create Dictionary {Word: ID}, where ID >= 1\n",
    "    word_id = { k: e for e, (k, v) in enumerate(word_id, start = 1)}\n",
    "    word_id[\"EOS\"] = 0\n",
    "\n",
    "    # Create a reverse Dictionary {ID: Word} \n",
    "    id_word = { v: k for k, v in word_id.items()}\n",
    "    \n",
    "    print('---- Image preprocessing ----')\n",
    "    \n",
    "    # image preprocessing (embedding using VGG16)\n",
    "    VGG = VGG16(weights='imagenet')\n",
    "    VGG._layers.pop()\n",
    "    VGG = Model(inputs=VGG.inputs, outputs=VGG.layers[-1].output)\n",
    "\n",
    "    # assign 1 text data (list of words) per 1 image\n",
    "    X1 = [] # image vector output embedded by VGG16 \n",
    "    X2 = [] # text vector\n",
    "    y = []  # predict next word \n",
    "\n",
    "    for i,(boke,url,directory) in enumerate(zip(w_ys, data.odai_photo_url, data.bokekan_id)):\n",
    "        try:\n",
    "            # 画像ロード\n",
    "            path = os.path.join(processing_bokekan_contents, directory)\n",
    "            img = load_img(path+url, target_size=(224, 224))\n",
    "            x = np.array(img, dtype=np.float32)/255\n",
    "            x = VGG(reshape(x, [1, 224, 224, 3])) \n",
    "\n",
    "        except:\n",
    "            print(\"Cannot load\", path+url)\n",
    "            continue\n",
    "\n",
    "        # 0-padding. set the next word to predict. \n",
    "        for i2,key in enumerate(boke):\n",
    "            if key != \"BOS\":\n",
    "                y.append(word_id[key])\n",
    "            if key == \"EOS\":\n",
    "                break\n",
    "\n",
    "            x2k = []\n",
    "            X1.append(reshape(x, [-1])) \n",
    "            word = boke[:i2+1]\n",
    "            x2k.extend([0 for n in range(max_word - len(word))])\n",
    "            for w in word:\n",
    "                x2k.append(word_id[w])\n",
    "            X2.append(x2k)\n",
    "\n",
    "    X1 = stack(X1)\n",
    "    X2 = stack(X2)\n",
    "    \n",
    "    print('---- Save data ----')\n",
    "    \n",
    "    vocabulary_size = max(map(int, id_word.keys()))+1\n",
    "\n",
    "    # save metadata for training/inference, dictionary for inference\n",
    "    with open(os.path.join(processing_bokekan_metadata, 'param.json'), 'w') as f:\n",
    "        json.dump({'max_word': max_word, 'vocabulary_size': vocabulary_size}, f)\n",
    "    pd.DataFrame(word_id.items()).to_csv(os.path.join(processing_bokekan_metadata, 'word_id_bokete_data.csv'), index=False)\n",
    "    pd.DataFrame(id_word.items()).to_csv(os.path.join(processing_bokekan_metadata, 'id_word_bokete_data.csv'), index=False)\n",
    "    \n",
    "    # save training data\n",
    "    np.savez(os.path.join(processing_output, 'X1.npz'), X1.numpy())\n",
    "    np.savez(os.path.join(processing_output, 'X2.npz'), X2.numpy())\n",
    "    np.savez(os.path.join(processing_output, 'y.npz'), np.array(y, dtype=np.int))\n",
    "    \n",
    "    print('---- Finished ----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above processing job will upload the training data to Amazon S3. Let's retrieve the path for the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = tf_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"training\":\n",
    "        preprocessed_training_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"bokekan_metadata\":\n",
    "        preprocessed_bokekan_metadata = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"bokekan_contents\":\n",
    "        preprocessed_bokekan_contents = output[\"S3Output\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve parameters\n",
    "!aws s3 cp {preprocessed_bokekan_metadata}/param.json ./\n",
    "\n",
    "with open('./param.json') as json_file:\n",
    "    param = json.load(json_file)\n",
    "    print(param)\n",
    "\n",
    "max_word = param['max_word']\n",
    "vocabulary_size = param['vocabulary_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In this tutorial, we train a deep CNN+LSTM to learn a image captioning task with the bokekan dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure metrics\n",
    "\n",
    "In addition to running the training job, Amazon SageMaker can retrieve training metrics directly from the logs and send them to CloudWatch metrics. Here, we define metrics we would like to observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'train:loss', 'Regex': '.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'train:accuracy', 'Regex': '.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'sec/steps', 'Regex': '.* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+'} \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a baseline training job on SageMaker\n",
    "\n",
    "The SageMaker Python SDK's `sagemaker.tensorflow.TensorFlow` estimator class makes it easy for us to interact with SageMaker. Here, we create one to configure a training job. Some parameters worth noting:\n",
    "\n",
    "* `entry_point`: our training script (adapted from [this Keras example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)).\n",
    "* `metric_definitions`: the metrics (defined above) that we want sent to CloudWatch.\n",
    "* `train_instance_count`: the number of training instances. Here, we set it to 1 for our baseline training job.\n",
    "\n",
    "For more details about the TensorFlow estimator class, see the [API documentation](https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {'epochs': 10, 'batch-size': 256, 'max_word': max_word, 'vocabulary_size': vocabulary_size}\n",
    "tags = [{'Key': 'Project', 'Value': 'bokete'}]\n",
    "\n",
    "estimator = TensorFlow(entry_point='bokete_keras.py',\n",
    "                       source_dir='./script/training', \n",
    "                       metric_definitions=metric_definitions,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       framework_version='2.3',\n",
    "                       py_version='py37',\n",
    "                       instance_count=1,\n",
    "                       instance_type='ml.g4dn.2xlarge',\n",
    "                       base_job_name='bokete-tf-keras-training',\n",
    "                       tags=tags)\n",
    "\n",
    "estimator.fit(preprocessed_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our estimator, we call `fit()` to start the SageMaker training job and pass the inputs that we uploaded to Amazon S3 earlier. We pass the inputs as a dictionary to define different data channels for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the job training metrics\n",
    "\n",
    "We can now view the metrics from the training job directly in the SageMaker console.  \n",
    "\n",
    "Log into the [SageMaker console](https://console.aws.amazon.com/sagemaker/home), choose the latest training job, and scroll down to the monitor section. Alternatively, the code below uses the region and training job name to generate a URL to CloudWatch metrics.\n",
    "\n",
    "Using CloudWatch metrics, you can change the period and configure the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "cw_url = parse.urlunparse((\n",
    "    'https',\n",
    "    '{}.console.aws.amazon.com'.format(region),\n",
    "    '/cloudwatch/home',\n",
    "    '',\n",
    "    'region={}'.format(region),\n",
    "    'metricsV2:namespace=/aws/sagemaker/TrainingJobs;dimensions=TrainingJobName;search={}'.format(estimator.latest_training_job.name),\n",
    "))\n",
    "\n",
    "display(Markdown('CloudWatch metrics: [link]({}). After you choose a metric, '\n",
    "                 'change the period to 1 Minute (Graphed Metrics -> Period).'.format(cw_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "After we train our model, we can deploy it to a SageMaker Endpoint, which serves prediction requests in real-time. To do so, we simply call `deploy()` on our estimator, passing in the desired number of instances and instance type for the endpoint.\n",
    "\n",
    "Because we're using TensorFlow Serving for deployment, our training script saves the model in TensorFlow's SavedModel format.  \n",
    "\n",
    "We don't need accelerated computing power for inference, so let's switch over to a <b>ml.m4.xlarge</b> instance type. \n",
    "\n",
    "For more information about deploying Keras and TensorFlow models in SageMaker, see [this blog post](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the endpoint\n",
    "\n",
    "To verify the that the endpoint is in service, we generate some random data in the correct shape and get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow import reshape, stack, convert_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_image = np.random.randn(1,4096)\n",
    "dummy_text = np.random.randn(1,max_word)\n",
    "payload = {\"inputs\": {\"image\": dummy_image.tolist(), \"text\": dummy_text.tolist()}}\n",
    "predictor.predict(payload)['outputs'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the test dataset for predictions. We will setup for inference preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models              import Model\n",
    "from tensorflow.keras.applications.vgg16  import VGG16 \n",
    "\n",
    "VGG = VGG16(weights='imagenet')\n",
    "VGG._layers.pop()\n",
    "VGG = Model(inputs=VGG.inputs, outputs=VGG.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary\n",
    "id_word = pd.read_csv(os.path.join(preprocessed_bokekan_metadata, 'id_word_bokete_data.csv'), index_col=0).to_dict()['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the first word from a distribution\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find your favorite image from https://bokete.jp/boke/select, then copy&paste the image url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://d2dcan0armyq93.cloudfront.net/photo/odai/600/2182ab815bdb2abae3b67e723861ac08_600.jpg'\n",
    "# image_url = 'https://d2dcan0armyq93.cloudfront.net/photo/odai/600/0235436e0d5d54c882f485e367f68fe6_600.jpg'\n",
    "# image_url = 'https://d2dcan0armyq93.cloudfront.net/photo/odai/600/db6e0eb2dd26330e1884d0812d78e783_600.jpg'\n",
    "\n",
    "!wget {image_url} ./\n",
    "image_path = image_url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please specify the `image_id` here.\n",
    "# image_id = 256\n",
    "# data = pd.read_csv(os.path.join(preprocessed_bokekan_metadata, 'boke.csv'))\n",
    "# image_path = os.path.join(data_path, bokekan, data.odai_photo_url[image_id][1:])\n",
    "# print(image_path)\n",
    "\n",
    "# Preprocess test image array\n",
    "img = load_img(image_path, target_size=(224, 224))\n",
    "x = np.array(img)/255\n",
    "\n",
    "x = VGG(reshape(x, [1, 224, 224, 3]))\n",
    "X_test_image = np.array(x)\n",
    "\n",
    "# Prepare empty text array\n",
    "x2 = np.zeros([1, max_word])\n",
    "x2[0, -1] = 1. # set {1: 'Beginning Of Sentence (BOS)'} at the beginning \n",
    "X_test_text = np.array(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, we can use it for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_length = max_word\n",
    "for i in range(max_length):\n",
    "    payload = {\"inputs\": {\"image\": X_test_image.tolist(), \"text\": X_test_text.tolist()}}\n",
    "    outputs = np.array(predictor.predict(payload)['outputs'])\n",
    "\n",
    "    # sampling the initial word\n",
    "    if i == 0:\n",
    "        X_test_text = np.append(X_test_text, sample(outputs[0], 1.0))[1:].reshape(1,-1)\n",
    "\n",
    "    elif outputs.argmax() == 0:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        X_test_text = np.append(X_test_text, outputs.argmax())[1:].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the predictions, we calculate our model accuracy and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "boke = ''.join(itemgetter(*X_test_text[0].tolist())(id_word)).split('BOS')[-1]\n",
    "\n",
    "plt.imshow(img)\n",
    "print(boke) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aided by the colors of the heatmap, we can use this confusion matrix to understand how well the model performed for each label.print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "To avoid incurring extra charges to your AWS account, let's delete the endpoint we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p38)",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
